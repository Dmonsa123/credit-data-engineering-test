import pandas as pd             # Librer√≠a para manejar tablas de datos (DataFrames).
import os                      # Librer√≠a para interactuar con carpetas y rutas de tu PC.
from datetime import datetime   # Para capturar la fecha y hora exacta del sistema.

def procesar_bronze():
    # Definimos d√≥nde est√°n los CSV (input) y d√≥nde guardaremos el Parquet (bronze).
    path_entrada = "data/input/"
    path_salida = "data/bronze/"
    
    # os.makedirs crea la carpeta 'data/bronze'. 
    # exist_ok=True evita que el programa falle si la carpeta ya estaba creada.
    os.makedirs(path_salida, exist_ok=True)

    print("--- üì• Iniciando Capa Bronze (Procesamiento Batch) ---")
    #para sacar el icono windows + v

    try:
        # Verificamos si la carpeta de entrada existe antes de intentar leerla.
        if not os.path.exists(path_entrada):
            print(f"‚ö†Ô∏è La carpeta {path_entrada} no existe.")
            return # Detiene la funci√≥n si no hay nada que leer.

        # LIST COMPREHENSION: Crea una lista con los nombres de archivos que terminan en '.csv'.
        # f es cada nombre de archivo que os.listdir encuentra en la carpeta.
        archivos = [f for f in os.listdir(path_entrada) if f.endswith('.csv')]
        
        # Si la lista est√° vac√≠a, avisamos y salimos.
        if not archivos:
            print("‚ö†Ô∏è No hay archivos en la carpeta de entrada.")
            return

        # Preparamos una lista vac√≠a para ir guardando los pedazos de datos.
        lista_dfs = []
        
        # Iniciamos un ciclo para procesar cada archivo CSV encontrado.
        for archivo in archivos:
            # Construimos la ruta completa (ej: data/input/batch_0.csv).
            ruta_full = os.path.join(path_entrada, archivo)
            
            # Cargamos el CSV actual en una tabla temporal (df_temp).
            df_temp = pd.read_csv(ruta_full)
            
            # --- A√ëADIR METADATOS (Auditor√≠a) ---
            # Creamos una columna con la fecha y hora de este preciso instante.
            # .floor('us') redondea a microsegundos para que Spark no tenga problemas de precisi√≥n.
            df_temp['ingestion_timestamp'] = pd.to_datetime(datetime.now()).floor('us')
            
            # Guardamos el nombre del archivo de origen para saber de d√≥nde vino el dato.
            df_temp['source_file'] = archivo
            
            # Agregamos este DataFrame con sus nuevas columnas a nuestra lista colectora.
            lista_dfs.append(df_temp)
            print(f"‚úîÔ∏è Le√≠do: {archivo}")

        # 2. CONSOLIDAR DATOS
        # pd.concat toma todos los DataFrames de la lista y los "pega" uno debajo del otro.
        # ignore_index=True hace que la numeraci√≥n de filas sea continua (0, 1, 2... hasta el final).
        df_bronze = pd.concat(lista_dfs, ignore_index=True)

        # 3. GUARDAR EN PARQUET
        # Definimos el nombre del archivo final unificado.
        output_file = os.path.join(path_salida, "events_bronze.parquet")
        
        # EL "FIX" PARA SPARK:
        # Guardamos la tabla consolidada en formato binario Parquet.
        df_bronze.to_parquet(
            output_file, 
            index=False,               # No guardamos la columna de √≠ndices de filas.
            engine='pyarrow',          # Usamos el motor PyArrow (necesario para manejar tipos complejos).
            coerce_timestamps='us',    # TRUCO: Convierte tiempos a microsegundos (formato nativo de Spark).
            allow_truncated_timestamps=True # Permite eliminar los nanosegundos sobrantes sin dar error.
        )
        
        # Mensajes finales de √©xito con estad√≠sticas.
        print(f"\n‚úÖ Capa Bronze completada con √©xito.")
        print(f"üìä Total registros: {len(df_bronze)}")
        print(f"üìÅ Destino: {output_file}")

    except Exception as e:
        # Si algo en el bloque 'try' falla, se ejecuta esto y te dice qu√© sali√≥ mal.
        print(f"‚ùå Error en Bronze: {e}")

# Punto de entrada del script.
if __name__ == "__main__":
    procesar_bronze()