import os
import sys
from pyspark.sql import SparkSession
# Importamos funciones matem√°ticas espec√≠ficas de Spark
from pyspark.sql.functions import col, sum, count, avg, round

def procesar_gold():
    # --- 1. CONFIGURACI√ìN DE ENTORNO (Hacks para Windows) ---
    if "SPARK_HOME" in os.environ: del os.environ["SPARK_HOME"]
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

    spark = SparkSession.builder \
        .master("local[*]") \
        .appName("Capa_Gold") \
        .config("spark.sql.parquet.datetimeRebaseModeInRead", "LEGACY") \
        # ESTA L√çNEA ES CLAVE: Evita errores con el sistema de archivos de Hadoop en Windows
        # Obliga a Spark a usar el sistema de archivos local de forma "cruda" (RawLocal)
        .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem") \
        .getOrCreate()

    try:
        print("üèÜ Iniciando proceso Capa Gold...")
        
        # --- 2. VALIDACI√ìN DE ORIGEN ---
        path_silver = "data/silver"
        if not os.path.exists(path_silver):
            # Si Silver no corri√≥, Gold no tiene nada que hacer. 
            # Esto es un control de seguridad b√°sico.
            print(f"‚ö†Ô∏è La carpeta {path_silver} no existe. Creando dummy o revisando...")
            return

        # Leemos los datos limpios de la capa anterior.
        df_silver = spark.read.parquet(path_silver)

        # --- 3. EL CORAZ√ìN DEL NEGOCIO (Agregaciones) ---
        print("üìä Calculando m√©tricas por regi√≥n...")
        
        # groupBy("region"): Junta todos los registros por su zona geogr√°fica.
        reporte_region = df_silver.groupBy("region").agg(
            # count: ¬øCu√°ntos cr√©ditos otorgamos en esa regi√≥n?
            count("loan_id").alias("total_prestamos"),
            
            # sum: ¬øCu√°nto dinero nos deben en total? (Redondeado a 2 decimales)
            round(sum("outstanding_balance"), 2).alias("deuda_total"),
            
            # avg: ¬øCu√°l es el inter√©s promedio que estamos cobrando ah√≠?
            round(avg("interest_rate"), 4).alias("tasa_promedio")
        ).orderBy(col("deuda_total").desc()) # Ponemos arriba la regi√≥n que m√°s nos debe.

        # --- 4. ALMACENAMIENTO DEL PRODUCTO FINAL ---
        # Guardamos el reporte final en la carpeta Gold.
        # mode("overwrite"): Si ya exist√≠a un reporte anterior, lo reemplaza por el m√°s reciente.
        reporte_region.write.mode("overwrite").parquet("data/gold/reporte_regional")
        
        print("‚úÖ Capa Gold completada exitosamente!")
        
        # Imprime el resumen final en la consola (formato tabla).
        reporte_region.show()

    except Exception as e:
        print(f"‚ùå Error en Gold: {e}")
    finally:
        # IMPORTANTE: Apagar el motor Spark para liberar memoria RAM.
        spark.stop()

if __name__ == "__main__":
    procesar_gold()